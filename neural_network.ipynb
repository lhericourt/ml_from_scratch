{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = data['data']\n",
    "y = data['target'].reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 30)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 1)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(10, 10, 10, 10, 10), learning_rate='constant',\n",
       "              learning_rate_init=10, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=42, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mlp.fit(X_test, y_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy on train : 93.2%\n",
      "Standard deviation on train : 1.3%\n",
      "Mean accuracy on test : 92.4%\n",
      "Standard deviation on test : 1.7%\n"
     ]
    }
   ],
   "source": [
    "nb_splits = 5\n",
    "skf = StratifiedKFold(n_splits=nb_splits, shuffle=True, random_state=42)\n",
    "mlp = MLPClassifier((50, 50, 50, 50, 50, 50, ), activation='tanh', solver='lbfgs', \n",
    "                    learning_rate='constant', learning_rate_init=0.01, random_state=42, verbose=True)\n",
    "score_train = list()\n",
    "score_test = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index].ravel(), y[test_index].ravel()\n",
    "    mlp.fit(X_train, y_train)\n",
    "    score_train.append(accuracy_score(y_train, mlp.predict(X_train)))\n",
    "    score_test.append(accuracy_score(y_test, mlp.predict(X_test)))\n",
    "\n",
    "    \n",
    "print(\"Mean accuracy on train : {0:.1f}%\".format(100 * np.mean(score_train)))\n",
    "print(\"Standard deviation on train : {0:.1f}%\".format(100 * np.std(score_train)))\n",
    "\n",
    "print(\"Mean accuracy on test : {0:.1f}%\".format(100 * np.mean(score_test)))\n",
    "print(\"Standard deviation on test : {0:.1f}%\".format(100 * np.std(score_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i is the number of the layer\n",
    "$$A_{i} = g_{i}(Z_{i})$$\n",
    "$$Z_{i} = A_{i-1}W_{i}.T + B_{i}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- last layer (L)\n",
    "$$dZ_{L} = A_{L} - Y$$\n",
    "\n",
    "\n",
    "- other layers (i)\n",
    "$$dZ_{i} = W_{i+1}^{T}dZ_{i+1} * g_{i}'(Z_{i})$$\n",
    "\n",
    "\n",
    "- All layers\n",
    "$$dW_{i} = \\frac{1}{n} * dZ_{i}A_{i-1}^{T}$$\n",
    "$$db_{i} =  mean(dZ_{i})$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "def tanh_derivative(X):\n",
    "    return 1 - np.tanh(X) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    seed = 42\n",
    "    \n",
    "    def __init__(self, idx, n_neurons, input_shape):\n",
    "        self.idx = idx\n",
    "        self.n_neurons = n_neurons\n",
    "        self.W = np.random.RandomState(Layer.seed).normal(size=(n_neurons, input_shape[1]))\n",
    "        self.b = np.ones((n_neurons, 1))\n",
    "        self.Z = None\n",
    "        self.output = None\n",
    "        self.dZ = None\n",
    "    \n",
    "    def forward_propagation(self, input_layer, a_function):\n",
    "        self.Z = np.dot(input_layer, self.W.T) + self.b.T\n",
    "        self.output = a_function(self.Z)\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward_propagation(self, W_next, dZ_next, output_previous, derivative_a_function, learning_rate):\n",
    "\n",
    "        self.dZ = np.dot(dZ_next, W_next) * derivative_a_function(self.Z)    \n",
    "        dW = np.dot(self.dZ.T, output_previous) / self.dZ.shape[0]\n",
    "        db = np.sum(self.dZ.T, axis=1, keepdims=True) / self.dZ.shape[0]\n",
    "        \n",
    "        self.W = self.W - learning_rate * dW\n",
    "        self.b = self.b - learning_rate * db\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    def backward_propagation_last_layer(self, y, output_previous, learning_rate):\n",
    "        y_pred = self.output\n",
    "        self.dZ = y_pred - y\n",
    "        \n",
    "        dW = np.dot(self.dZ.T, output_previous) / self.dZ.shape[0]\n",
    "        db = np.sum(self.dZ.T, axis=1, keepdims=True) / self.dZ.shape[0]\n",
    "        self.W = self.W - learning_rate * dW\n",
    "        self.b = self.b - learning_rate * db\n",
    "        \n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self, n_layers, n_neurons, learning_rate, max_epoch):\n",
    "        self.n_layers = n_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epoch = max_epoch\n",
    "        self.all_layers = list()\n",
    "        \n",
    "    def forward_propagation(self, X):\n",
    "        input_layer = X\n",
    "        \n",
    "        for layer in self.all_layers[: -1]:\n",
    "            input_layer = layer.forward_propagation(input_layer, np.tanh)\n",
    "        output = self.all_layers[-1].forward_propagation(input_layer, sigmoid)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def backward_propagation(self, X, y):\n",
    "        \n",
    "        for i, layer in enumerate(reversed(self.all_layers)):\n",
    "            reversed_i = self.n_layers - i - 1\n",
    "            \n",
    "            if reversed_i > 0:\n",
    "                output_previous = self.all_layers[reversed_i - 1].output\n",
    "            else:\n",
    "                output_previous = X\n",
    "            \n",
    "            if reversed_i == self.n_layers - 1:\n",
    "                layer.backward_propagation_last_layer(y, output_previous, self.learning_rate)\n",
    "            else:\n",
    "                W_next = self.all_layers[reversed_i + 1].W\n",
    "                dZ_next = self.all_layers[reversed_i + 1].dZ\n",
    "                layer.backward_propagation(W_next, dZ_next, output_previous, tanh_derivative, self.learning_rate)\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    def compute_loss(self, y_pred, y):\n",
    "        total_loss = np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "        return - total_loss\n",
    "    \n",
    "    def init_layers(self, input_shape): \n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            if i == self.n_layers - 1:\n",
    "                layer_i = Layer(i, 1, input_shape)\n",
    "            else:\n",
    "                layer_i = Layer(i, self.n_neurons, input_shape)\n",
    "            input_shape = (input_shape[0], self.n_neurons)\n",
    "            self.all_layers.append(layer_i)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.init_layers(X.shape)\n",
    "        stop_tolerance = 1e-4\n",
    "        stop_nb_iter = 5\n",
    "            \n",
    "        saved_losses = list()\n",
    "        for i in range(self.max_epoch):\n",
    "            y_pred = self.forward_propagation(X)\n",
    "            self.backward_propagation(X, y)\n",
    "            loss = self.compute_loss(y_pred, y)\n",
    "            \n",
    "            if saved_losses and abs(loss - saved_losses[-1]) < stop_tolerance:\n",
    "                nb_iter_in_tolerance += 1\n",
    "            else:\n",
    "                nb_iter_in_tolerance = 0\n",
    "            \n",
    "            if nb_iter_in_tolerance >= stop_nb_iter:\n",
    "                print(\"The Neural network has converged in {} epochs\".format(i))\n",
    "                print(\"The Neural network Loss is : {}\".format(loss))\n",
    "                break\n",
    "            if i == self.max_epoch - 1:    \n",
    "                print(\"The Neural network has not converged\")\n",
    "            saved_losses.append(loss)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        y_pred = np.rint(self.forward_propagation(X))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Neural network has converged in 1232 epochs\n",
      "The Neural network Loss is : 0.26738905379209127\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(4, 50, learning_rate=0.001, max_epoch=10000)\n",
    "nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9298245614035088"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = nn.predict(X_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9032967032967033"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = nn.predict(X_train)\n",
    "accuracy_score(y_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
